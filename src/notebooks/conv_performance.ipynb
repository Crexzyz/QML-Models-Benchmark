{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e41cc2",
   "metadata": {},
   "source": [
    "# QuantumConv2D Performance Benchmarks\n",
    "\n",
    "This notebook benchmarks the performance of three implementations of a Quantum Convolutional Layer:\n",
    "1. **Sequential**: Standard loop-based implementation on CPU\n",
    "2. **Batched (CPU)**: Vectorized implementation running on CPU\n",
    "3. **Batched (GPU)**: Vectorized implementation running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8543bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "from qml.layers import QuantumConv2D, BatchedQuantumConv2D, BatchedGPUQuantumConv2D\n",
    "from qml.ansatz.standard import StandardQCNNAnsatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ba3e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device for GPU model: cuda\n",
      "Batch Size: 16, Image Size: 28x28\n"
     ]
    }
   ],
   "source": [
    "# Setup Device and Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Target device for GPU model: {device}\")\n",
    "\n",
    "batch_size = 16  # Batch size for testing\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "n_qubits = 4\n",
    "\n",
    "print(f\"Batch Size: {batch_size}, Image Size: {height}x{width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475dc5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Create Random Input Data\n",
    "# Fix seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "# GPU Version needs input on the device\n",
    "x_gpu = x.clone().to(device)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4973dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing layers...\n",
      "Using lightning.qubit device with 'ry' encoding, StandardQCNNAnsatz, measurement=PauliZ\n",
      "Using lightning.qubit device with 'ry' encoding, StandardQCNNAnsatz, measurement=PauliZ\n",
      "Using default.qubit device with 'ry' encoding, StandardQCNNAnsatz, measurement=PauliZ\n",
      "Layer initialization complete and weights synchronized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Layers\n",
    "ansatz = StandardQCNNAnsatz()\n",
    "\n",
    "print(\"Initializing layers...\")\n",
    "\n",
    "# 1. Sequential (Slow reference)\n",
    "layer_sequential = QuantumConv2D(\n",
    "    n_qubits=n_qubits, ansatz=ansatz, stride=1, encoding=\"ry\"\n",
    ")\n",
    "\n",
    "# 2. Batched CPU (Fast)\n",
    "layer_batched_cpu = BatchedQuantumConv2D(\n",
    "    n_qubits=n_qubits, ansatz=ansatz, stride=1, encoding=\"ry\"\n",
    ")\n",
    "\n",
    "# 3. Batched GPU (Fastest on NVIDIA)\n",
    "layer_batched_gpu = BatchedGPUQuantumConv2D(\n",
    "    n_qubits=n_qubits, ansatz=ansatz, stride=1, encoding=\"ry\"\n",
    ").to(device)\n",
    "\n",
    "# FORCE WEIGHTS TO BE IDENTICAL FOR FAIR COMPARISON\n",
    "with torch.no_grad():\n",
    "    layer_batched_cpu.q_params.data = layer_sequential.q_params.data.clone()\n",
    "    layer_batched_gpu.q_params.data = layer_sequential.q_params.data.to(device)\n",
    "\n",
    "print(\"Layer initialization complete and weights synchronized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d7e7366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sequential (Slow) Version (CPU)...\n",
      "Sequential Time: 55.1463 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test Sequential Implementation\n",
    "print(\"Running Sequential (Slow) Version (CPU)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_sequential = layer_sequential(x)\n",
    "\n",
    "end_time = time.time()\n",
    "time_sequential = end_time - start_time\n",
    "print(f\"Sequential Time: {time_sequential:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72dba373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Batched Version (CPU)...\n",
      "Batched CPU Time: 31.6674 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test Batched CPU Implementation\n",
    "print(\"Running Batched Version (CPU)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_batched_cpu = layer_batched_cpu(x)\n",
    "\n",
    "end_time = time.time()\n",
    "time_batched_cpu = end_time - start_time\n",
    "print(f\"Batched CPU Time: {time_batched_cpu:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371b190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Batched Version (cuda)...\n",
      "Batched GPU Time: 0.5908 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test Batched GPU Implementation\n",
    "# Note: This is the most critical benchmark for large scale training\n",
    "print(f\"Running Batched Version ({device})...\")\n",
    "\n",
    "# Sync CUDA before starting timer\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_batched_gpu = layer_batched_gpu(x_gpu)\n",
    "\n",
    "# Sync CUDA after execution to measure actual compute time\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "end_time = time.time()\n",
    "time_batched_gpu = end_time - start_time\n",
    "print(f\"Batched GPU Time: {time_batched_gpu:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f39b493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup Batched vs Sequential: 1.74x\n",
      "Speedup GPU vs Sequential:     93.34x\n",
      "Speedup GPU vs Batched CPU:    53.60x\n",
      "\n",
      "Max Diff Batched (CPU): 0.0\n",
      "Max Diff GPU:           3.5762786865234375e-07\n",
      ">> VALIDATION PASSED: All results are identical (within tolerance).\n"
     ]
    }
   ],
   "source": [
    "# Compare Results and Verify Correctness\n",
    "\n",
    "speedup_batched = time_sequential / time_batched_cpu\n",
    "speedup_gpu = time_sequential / time_batched_gpu\n",
    "speedup_batched_gpu = time_batched_cpu / time_batched_gpu\n",
    "\n",
    "print(f\"Speedup Batched vs Sequential: {speedup_batched:.2f}x\")\n",
    "print(f\"Speedup GPU vs Sequential:     {speedup_gpu:.2f}x\")\n",
    "print(f\"Speedup GPU vs Batched CPU:    {speedup_batched_gpu:.2f}x\")\n",
    "\n",
    "# Check correctness (Numerical drift is expected, but should be small)\n",
    "diff_batched = torch.abs(out_sequential - out_batched_cpu).max().item()\n",
    "diff_gpu = torch.abs(out_sequential - out_batched_gpu.cpu()).max().item()\n",
    "\n",
    "print(f\"\\nMax Diff Batched (CPU): {diff_batched}\")\n",
    "print(f\"Max Diff GPU:           {diff_gpu}\")\n",
    "\n",
    "# Validation Threshold\n",
    "if diff_batched < 1e-4 and diff_gpu < 1e-4:\n",
    "    print(\">> VALIDATION PASSED: All results are identical (within tolerance).\")\n",
    "else:\n",
    "    print(\">> VALIDATION FAILED: Results differ significantly!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
